#!/usr/bin/python
# -*- coding: utf-8 -*-

import httplib
#import re
#import sys
import os
#import Cookie
#import string, xbmc, xbmcgui, xbmcplugin, xbmcaddon
#-------------------------------
import urllib, urllib2, time, random
#from time import gmtime, strftime 
#from urlparse import urlparse 
#import json
import socket
socket.setdefaulttimeout(30)

#-----------------------------------------
#-----------------------------------------
siteUrl = 'nnmclub.to'
httpSiteUrl = 'http://' + siteUrl


# - ====================================== antizapret ====================================================
#import time, cookielib
#sid_file = os.path.join(xbmc.translatePath('special://temp/'), 'vpn.sid')
#cj = cookielib.FileCookieJar(sid_file) 
#hr  = urllib2.HTTPCookieProcessor(cj) 
#Lthread=[]


def ru(x):return unicode(x,'utf8', 'ignore')
def xt(x):return xbmc.translatePath(x)
def rt(x):#('&#39;','’'), ('&#145;','‘')
	L=[('&quot;','"'), ('&amp;',"&"),('&#133;','…'),('&#38;','&'),('&#34;','"'), ('&#39;','"'), ('&#145;','"'), ('&#146;','"'), ('&#147;','“'), ('&#148;','”'), ('&#149;','•'), ('&#150;','–'), ('&#151;','—'), ('&#152;','?'), ('&#153;','™'), ('&#154;','s'), ('&#155;','›'), ('&#156;','?'), ('&#157;',''), ('&#158;','z'), ('&#159;','Y'), ('&#160;',''), ('&#161;','?'), ('&#162;','?'), ('&#163;','?'), ('&#164;','¤'), ('&#165;','?'), ('&#166;','¦'), ('&#167;','§'), ('&#168;','?'), ('&#169;','©'), ('&#170;','?'), ('&#171;','«'), ('&#172;','¬'), ('&#173;',''), ('&#174;','®'), ('&#175;','?'), ('&#176;','°'), ('&#177;','±'), ('&#178;','?'), ('&#179;','?'), ('&#180;','?'), ('&#181;','µ'), ('&#182;','¶'), ('&#183;','·'), ('&#184;','?'), ('&#185;','?'), ('&#186;','?'), ('&#187;','»'), ('&#188;','?'), ('&#189;','?'), ('&#190;','?'), ('&#191;','?'), ('&#192;','A'), ('&#193;','A'), ('&#194;','A'), ('&#195;','A'), ('&#196;','A'), ('&#197;','A'), ('&#198;','?'), ('&#199;','C'), ('&#200;','E'), ('&#201;','E'), ('&#202;','E'), ('&#203;','E'), ('&#204;','I'), ('&#205;','I'), ('&#206;','I'), ('&#207;','I'), ('&#208;','?'), ('&#209;','N'), ('&#210;','O'), ('&#211;','O'), ('&#212;','O'), ('&#213;','O'), ('&#214;','O'), ('&#215;','?'), ('&#216;','O'), ('&#217;','U'), ('&#218;','U'), ('&#219;','U'), ('&#220;','U'), ('&#221;','Y'), ('&#222;','?'), ('&#223;','?'), ('&#224;','a'), ('&#225;','a'), ('&#226;','a'), ('&#227;','a'), ('&#228;','a'), ('&#229;','a'), ('&#230;','?'), ('&#231;','c'), ('&#232;','e'), ('&#233;','e'), ('&#234;','e'), ('&#235;','e'), ('&#236;','i'), ('&#237;','i'), ('&#238;','i'), ('&#239;','i'), ('&#240;','?'), ('&#241;','n'), ('&#242;','o'), ('&#243;','o'), ('&#244;','o'), ('&#245;','o'), ('&#246;','o'), ('&#247;','?'), ('&#248;','o'), ('&#249;','u'), ('&#250;','u'), ('&#251;','u'), ('&#252;','u'), ('&#253;','y'), ('&#254;','?'), ('&#255;','y'), ('&laquo;','"'), ('&raquo;','"'), ('&nbsp;',' ')]
	for i in L:
		x=x.replace(i[0], i[1])
	return x

def mfindal(http, ss, es):
	L=[]
	while http.find(es)>0:
		s=http.find(ss)
		e=http.find(es)
		i=http[s:e]
		L.append(i)
		http=http[e+2:]
	return L

def mfind(t,s,e):
	n = t.find(s)
	if n == -1: return ''
	r=t[n+len(s):]
	f = r.find(e)
	if f == -1: return ''
	r2=r[:f]
	return r2

def rulower(str):
	str=str.strip()
	str=xt(str).lower()
	str=str.replace('Й','й')
	str=str.replace('Ц','ц')
	str=str.replace('У','у')
	str=str.replace('К','к')
	str=str.replace('Е','е')
	str=str.replace('Н','н')
	str=str.replace('Г','г')
	str=str.replace('Ш','ш')
	str=str.replace('Щ','щ')
	str=str.replace('З','з')
	str=str.replace('Х','х')
	str=str.replace('Ъ','ъ')
	str=str.replace('Ф','ф')
	str=str.replace('Ы','ы')
	str=str.replace('В','в')
	str=str.replace('А','а')
	str=str.replace('П','п')
	str=str.replace('Р','р')
	str=str.replace('О','о')
	str=str.replace('Л','л')
	str=str.replace('Д','д')
	str=str.replace('Ж','ж')
	str=str.replace('Э','э')
	str=str.replace('Я','я')
	str=str.replace('Ч','ч')
	str=str.replace('С','с')
	str=str.replace('М','м')
	str=str.replace('И','и')
	str=str.replace('Т','т')
	str=str.replace('Ь','ь')
	str=str.replace('Б','б')
	str=str.replace('Ю','ю')
	return str

def lower(s):
	try:s=s.decode('utf-8')
	except: pass
	try:s=s.decode('windows-1251')
	except: pass
	s=s.lower().encode('utf-8')
	return s


def GET(target, referer='', post=None):
	try:
		req = urllib2.Request(url = target, data = post)
		req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)')
		req.add_header('accept-encoding', 'gzip')
		resp = urllib2.urlopen(req)
		if resp.info().get('Content-Encoding') == 'gzip':
			from StringIO import StringIO
			import gzip
			buf = StringIO(resp.read())
			f = gzip.GzipFile(fileobj=buf)
			http = f.read()
		else:
			http=resp.read()
		resp.close()
		return http
	except Exception, e:
		print e

def POST(target, post=None, referer=''):
	try:
		req = urllib2.Request(url = target, data = post)
		req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)')
		req.add_header('Referer', referer)
		#req.add_header('X-Requested-With', 'XMLHttpRequest')
		resp = urllib2.urlopen(req)
		http = resp.read()
		resp.close()
		http = http.decode(resp.info().get('Content-Type').split('charset=')[1], 'replace')
		http = http.encode('utf8')
#		print resp.geturl()
		return http
	except Exception, e:
		return ''

def get_list(hp):#, info):
	#title=info['title']
	#print '== get_list =='
	#print len(hp)
        body = mfind(hp, '<tbody>','</tbody>')
#	print body
	L=mfindal(body, '<tr class="prow', '</tr>')
#	print len(L)
	L2=[]
	for i in L: #[:2]:
#		print i
#		print '======='
		for s in i.splitlines():
#			print s
			if 'td title="" class=' in s:
				ttl = mfind(s, '<b>', '<')
				ttd = mfind(s, 'gensmall opened">', '</span')
			if '"nowrap" class="gensmall"><u>' in s:
				size=mfind(s, '</u>', '</td>').replace('&nbsp;', ' ').replace(',','.').strip()
			if 'title="Seeders"' in s:
				seeds = mfind(s, '<b>', '<')
			if 'download.php?id=' in s:
				url = mfind(s, 'href="','" rel')
#		print ttl
#		print ttd
#		print size
#		print seeds
#		print url
		curl=httpSiteUrl+'/forum/'+url
#		print curl
		if ttd: ttl = ttl+' '+ttd
		itm ={"sids":seeds,"size":size, "title":ttl,"url":curl}#xt()
#		if int(seeds) > 5:
		L2.append(itm)
	
	return L2



class Tracker:
	def __init__(self):
		pass

	def Search(self, info):
		#print '====== wriza.top ====='
		#tmc = time.strftime('%d%y%m')
		t = info['originaltitle']
		if isinstance(t, str): t = t.decode('utf8')

		""" #
	        string_to_search_converted = t.replace(' ', '+').encode('cp1251')
	        payload = {
	            'prev_sd' : 0,
	            'prev_a' : 0,
	            'prev_my' : 0,
	            'prev_n' : 0,
	            'prev_shc' : 0, # показывать колонку Категория
	            'prev_shf' : 1, # показывать колонку Форум
	            'prev_sha' : 1, # показывать колонку Автор
	            'prev_shs' : 0, # показывать колонку Скорость
	            'prev_shr' : 0, # показывать колонку Рейтинг
	            'prev_sht' : 0, # показывать колонку Спасибо
	            'f[]' : -1, # категории, где искать. '-1' = 'все имеющиеся'
	            'o' : 1, # 1 - зарегистрирован, 4 - скачан, 7 - размер
	            's' : 2, # 1 - по возрастанию, 2 - по убыванию
	            'tm' : -1, # 30 - посл месяц, 7 - за посл неделю, -1 - за все время
	            'shf' : 1,
	            'sha' : 1,
	            'ta' : -1,
	            'sns' : -1,
	            'sds' : -1,
	            'nm' : string_to_search_converted,
	            'pn' : '',
	            'submit' : 'Поиск'
	        }

		url=httpSiteUrl+'/forum/tracker.php'
#		print payload
		post=urllib.urlencode(payload)
		"""

		from drivers.rutracker import RuTracker
		r  = RuTracker()
		se = r.search(t)['data']
		id = ''
#		print se
		for i in se:
			id += str(i['id'])+','
		id = id.strip(', ')
		if id != '': gttd = eval(GET('http://api.rutracker.org/v1/get_tor_topic_data?by=topic_id&val='+urllib.quote_plus(str(id))))
#		print gttd
		Lout = []
		for i in se:
			d = gttd['result'][str(i['id'])]
			ttl = i['name'].encode('utf8')
			param = {
				'dn': ttl,
				'tr': ["http://bt.t-ru.org/ann","http://bt2.t-ru.org/ann","http://bt3.t-ru.org/ann","http://bt4.t-ru.org/ann","http://bt5.t-ru.org/ann","http://bt2.t-ru.org/ann?magnet","http://retracker.local/announce"],
			}
			magnet = 'magnet:?xt=urn:btih:'+d['info_hash'].lower()+'&'+urllib.urlencode(param, True)
			Lout.append({'sids':d['seeders'], 'size': int(d['size']), 'title': ttl, 'url':magnet})

#		print post
#		hp=POST(url, post)
#		file('nnmclub_s.txt', 'wb').write(hp)
#		hp = file('nnmclub_s.txt', 'rb').read()
#		Lout=get_list(hp) #, info)
		
		return Lout

	def Query(self, query):
		try:
			return self.Search({'originaltitle': query})
		except Exception as e:
			print e
			return []

if __name__ == '__main__':
	print Tracker().Query('ария')
